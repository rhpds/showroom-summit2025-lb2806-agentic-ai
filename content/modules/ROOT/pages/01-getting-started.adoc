= 1. Getting Started and Environment Setup

Welcome to the first module! Our goal here is to get you set up in the lab environment and run a few basic checks. This will ensure everything is working correctly and familiarize you with the tools before we dive into creating our first AI Agent in the next module.

This entire lab takes place on a Red Hat Enterprise Linux (RHEL) 9.5 Virtual Machine (VM) equipped with an NVIDIA L4 Graphics Processing Unit (GPU). While we'll be working directly on RHEL for this lab, we'll also discuss how these AI applications would typically be deployed, managed, and scaled in a production environment using Red Hat OpenShift.

[NOTE]
====
For this lab, you'll be taking on the persona of a developer. This means we'll be working with hardware and tools commonly used during the development phase. For instance, we'll use `ollama` as a runtime for our Large Language Model (LLM). An LLM is a type of artificial intelligence model that has been trained on vast amounts of text data to understand and generate human-like text.

In a production setting, you would likely use a more robust, enterprise-grade runtime like `vLLM`, which is part of Red Hat OpenShift AI. We'll explore these distinctions in more detail in Module X.
====

As is common in Python-based AI development, you will work within a Python Virtual Environment (venv). For this lab, we are using Python version `3.12`.

[NOTE]
====
Python developers, both in AI and other software development fields, frequently use virtual environments. A `venv` isolates a project's specific set of Python packages and dependencies (like the ones you might list in a `requirements.txt` file, often generated using `pip freeze > requirements.txt`). This isolation prevents conflicts between different projects and ensures reproducibility. These `requirements.txt` files often become key artifacts in the CI/CD (Continuous Integration/Continuous Deployment) pipeline, used to build the container images that run in production environments like Red Hat OpenShift.
====

.Your Lab Environment Overview
[cols="1,2"]
|===
|Component |Details

|Virtual Environment
|`~/venv`, Python 3.12

|Operating System
|Red Hat Enterprise Linux (RHEL) 9.5, with NVIDIA drivers and CUDA toolkit installed. (CUDA is a parallel computing platform and programming model developed by NVIDIA for general computing on its own GPUs.)

|GPU
|NVIDIA L4 (24GB VRAM)

|LLM Runtime (Development)
|Ollama (which uses `lamacpp` internally).
(In production, you would typically use Red Hat OpenShift AI with a runtime like `vLLM`.)
|===

[TIP]
====
You can monitor your GPU's utilization at any time by typing the `nvtop` command in a terminal. 

Your lab interface provides two terminals in the "Terminal Tab," but you can open additional terminals as needed directly within the JupyterLab interface.

// image::jupyter-ttys.png[Caption: Adding New Terminals in JupyterLab Interface]
====



== JupyterLab and Jupyter Notebooks 

[NOTE]
====
This lab runs in JupyterLab, an interactive development environment you'll access directly through your web browser. Think of JupyterLab as a workbench that's excellent for hands-on experimentation, which is especially useful in AI development.

Within JupyterLab, we'll be using Jupyter Notebooks. A Notebook is like an interactive document where you can:

* Write and execute Python code in small, manageable cells.
* Instantly see the output from your code.
* Easily modify code and vars and re-execute 
* Combine code with explanatory text (like this!), images, and charts, all in one place.
This combination makes Notebooks ideal for learning new concepts and testing ideas step-by-step.

.Quick interactive introduction to using and customizing Jupyter Lab (recommended)
++++
<iframe 
  src="https://demo.arcade.software/eF1CtDuuD4hKWspBmiYR?
  
  embed&embed_mobile=tab&embed_desktop=inline&show_copy_link=true"
  embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true
  width="100%" 
  height="600px" 
  frameborder="0" 
  allowfullscreen
  muted>
</iframe>
++++

====

// [WARNING]
// ====
// *TODO*: Insert an image here illustrating the lab environment, perhaps showing the RHEL desktop, a terminal window indicating GPU driver status (e.g., `nvidia-smi`), or the CUDA version.
// ====

== Starting the Large Language Model (LLM)

To begin, let's interact with the LLM. On the right side of your screen, within the lab user interface, please switch from the "JupyterLab" tab to the "Terminals" tab.

Perform the following commands in the *top* terminal window:

.   **Check the status of your LLM runtime (`ollama`).**
+
    `ollama` is a user-friendly tool that simplifies running LLMs locally. It acts as a wrapper for the underlying LLM runtime, which in this case is `llamacpp`.
+
[source,sh,role=execute]
----
ollama ps
----
+
    This command shows any models currently loaded and running via `ollama`. If no models are running, the output will be minimal, similar to this:
+
.Sample Output (if no models are running)
[source,text]
----
NAME    ID    SIZE    PROCESSOR    UNTIL
----

.   **List the LLM models available locally.**
+
    Similar to how container tools like Podman or Docker work with images, `ollama` allows you to download (or "pull") new LLM models using the `ollama pull <model_name>` command. Let's see what we have:
+
[source,sh,role=execute]
----
ollama ls
----
+
.Sample Output
[source,text]
----
NAME                         ID              SIZE      MODIFIED     
llama3.2:3b-instruct-fp16    195a8c01d91e    6.4 GB    10 hours ago   
----
+

Different LLMs possess distinct characteristics and capabilities. The `mistral-small:latest` model has been selected for this lab partly because it's optimized for "Tool Use." As we'll discover, "Tool Use" (also known as function calling) is a crucial feature for AI Agents, allowing them to interact with external systems or data sources to perform tasks.
+

Let's briefly look at the details of our chosen model:
+

[cols="55%,20%,15%,10%"]
|===
|Model Name | Parameters | Size | Tool Calling?

|`llama3.2:3b-instruct-fp16`
|3.2 Billion
|`~6.4GB`
|Yes
|===
+

*Model Name* (`llama3.2:3b-instruct-fp16`):
+

`llama3.2`: This tells us we're using a specific version of the Llama family of models, in this case, version 3.2. Think of this like a software version (e.g., RHEL 9.4).
`3b``: This indicates the model has approximately 3.2 Billion parameters. We'll explain "parameters" next.
+

`instruct`: "Instruct" means this model has been specifically fine-tuned to understand and follow instructions, much like how you'd give commands in a Linux terminal. This makes it more suitable for tasks where you're asking it to do something specific, which is exactly what we want for our AI Agent.
+

`fp16`: This refers to "16-bit floating-point" precision.
+

*Parameters (3.2 Billion):
+

In the context of a Large Language Model (LLM), "parameters" are the internal variables the model learns during its training process. These are essentially the 'knowledge' and 'reasoning patterns' the model develops.
+

The ~6.4GB size is how much disk space the model files take up and gives an idea of the memory (RAM, or often GPU RAM) needed to load and run it.
+

The fp16 in the model name is crucial here. It stands for "16-bit floating-point precision." Standard models are often trained and stored at fp32 (32-bit precision).
+

*Tool Calling* (also known as function calling) is a powerful feature for AI Agents. It means we can define specific functions or 'tools' (which are essentially pieces of code) that the LLM can decide to use to get more information or perform actions.
+

Why it matters to you: For an operations-focused lab, this is extremely relevant. Imagine an agent that needs to:

* Check the current status of a service on a server.
* Fetch logs for a specific pod in OpenShift.
* Look up information in an internal knowledge base. 
+

Tool calling allows the LLM to say, "I need to run the get_pod_logs tool with these parameters." Our agent framework then executes that code and feeds the result back to the LLM. This allows the LLM to interact with the outside world and your systems in a structured way, going beyond just text generation.

.   **Launch the `llama3.2:3b-instruct-fp16` LLM using `ollama`.**
+

[source,sh,role=execute]
----
ollama run llama3.2:3b-instruct-fp16 --keepalive -1m
----
+
    The `--keepalive -1m` flag tells `ollama` to keep the model loaded in memory indefinitely until explicitly stopped (e.g., with `ollama stop lama3.2:3b-instruct-fp16`). Without this, `ollama` might unload the model after a period of inactivity (default is 5 minutes).
+

[NOTE]
====
In a production environment on OpenShift, you'd utilize a production-grade LLM serving solution like `vLLM` (as found in OpenShift AI). For development, tools like `ollama` are excellent because they simplify the setup and allow for easy use of quantized (smaller) LLMs on readily available GPUs. Red Hat also offers RHEL AI for deploying and managing AI models directly on RHEL.
====
+
    After running this command, you'll see a prompt appear, indicating the LLM is ready for direct interaction:
+
.Sample Output
[source,text]
----
>>>
----
+

[TIP]
====
You can now type questions or "prompts" directly to the LLM at the `>>>` prompt in this terminal. A "prompt" is the input you provide to an LLM to elicit a response.

While direct interaction is possible here, throughout this lab, we will primarily interact with the model programmatically using its API. `ollama` conveniently exposes an OpenAI-compatible API endpoint, typically on TCP port `11434`.

Feel free to ask the LLM a question now if you're curious! For example, try typing: `What are the benefits of using Linux?` and press Enter.
====

.   **(Optional) Interact with the LLM via its API using `curl`.**
+
    This step demonstrates how applications can communicate with the LLM. We'll use the `curl` command (a common tool for transferring data with URLs) to send a request to the `ollama` API endpoint.
+
    Open the *bottom* terminal (or a new terminal) for this command.
+
[source,sh,role=execute]
----
curl -s http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b-instruct-fp16",
  "prompt": "What is the capital of France?",
  "stream": false
}' | jq .response
----
+
    Let's break down this command:

    * `curl -s http://localhost:11434/api/generate`: Sends a request to the LLM's generation endpoint. The `-s` flag makes `curl` operate silently (no progress meter).
    * `-d '{...}'`: Specifies the data to send in the request body, formatted as a JSON object.
        * `"model": "lama3.2:3b-instruct-fp16"`: Tells `ollama` which LLM to use.
        * `"prompt": "What is the capital of France?"`: The question we're asking the LLM.
        * `"stream": false`: Instructs the API to send the entire response at once, rather than streaming it token by token.
    * `| jq .response`: The output from `curl` (which is a JSON string) is "piped" (`|`) to the `jq` command. `jq` is a command-line JSON processor. `.response` tells `jq` to extract the value associated with the "response" key from the JSON.
+

.Sample Output
[source,text]
----
"The capital of France is Paris."
----

== Optional: Graphically Monitor Your GPU

Let's see our GPU in action. If you haven't already, switch your focus to the *bottom* terminal.

.   **Start the GPU monitoring application `nvtop`.**
+

`nvtop` (NVIDIA top) is a command-line task monitor for NVIDIA GPUs, similar to how `top` or `htop` monitor CPU and system processes.

[source,sh,role=execute]
----
nvtop
----
+
    You should now see a display showing GPU utilization, memory usage, temperature, and other metrics.
+
[NOTE]
====
While not essential for completing the lab exercises, observing `nvtop` can be insightful. It provides a simple and effective way to confirm that your AI tasks are indeed utilizing the GPU and to get a sense of the resources being consumed.
====
+
    To see the GPU usage change, switch back to your *upper* terminal where `ollama run ...` is active (you should see the `>>>` prompt). Type a prompt that will require the LLM to generate a significant amount of text. For example:
+

[source,text,role=execute]
----
Tell me a short story about a robot exploring Mars.
----
+
    As the LLM processes this and generates the story, you should see activity increase in the `nvtop` display in your other terminal.

You are now set up and have confirmed the LLM is operational. We are ready to move on to the next module, where we'll begin building our first AI Agent!
