= Prompting

= Understanding AI Prompts: Chain of Thought (CoT) vs. ReAct

This module serves as a practical guide to two influential prompting techniques for Large Language Models (LLMs): Chain of Thought (CoT) and ReAct (Reason + Act). As AI becomes increasingly integrated into technical workflows, understanding how to effectively communicate instructions to these models is crucial, especially for Operations (Ops) professionals who are newer to AI and Python. We'll explore how these prompting strategies can enhance the reliability and transparency of AI-driven tasks, from simple Q&A to more complex interactions with external systems and tools.

We begin by setting up a Python environment to interact with an LLM, explaining each component from an Ops perspectiveâ€”covering library imports, API key management, model selection, and client initialization. The core of this notebook then delves into CoT prompting, demonstrating how guiding an LLM to "think step-by-step" can improve reasoning and provide clarity. Following this, we introduce ReAct, a more advanced technique that empowers LLMs to not only reason but also to propose and simulate "actions" (like tool usage) and process "observations," making them more suitable for dynamic, real-world operational scenarios. By comparing these methods, this guide aims to equip you with the foundational knowledge to leverage LLMs more effectively in your operational roles.

[NOTE]
====
For the rest of this module work through the annotated Jupyter Notebook `03-`
====


// Tools, APIs, and Microservices

// In Module 3 we will dive deeper into Agents and in particualer Tools and Tool Calling,  

// and how to use them in your applications. We will also look at the OpenAI API and how to use it to call LLMs and other microservices.

// == Tools and Tool Calling

// In this module we will look at the following: